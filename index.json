[{"authors":null,"categories":null,"content":"Alexey Tumanov is a tenure-track Assistant Professor of Computer Science at Georgia TEch since August 2019. His research interests include Systems for Machine Learning, specifically soft real-time ML support, resource management for distributed ML, distributed inference, distributed dataframe analytics, and latency-aware Neural Architecture Search (NAS). Prof. Tumanov completed his postdoc at the University of California, Berkeley with Ion Stoica and in close collaboration with several other co-directors of RISELab, including Joseph Gonzalez and Joe Hellerstein. During his PhD studies at Carnegie Mellon, Alexey focused on the development of the first declarative, heterogeneity-aware scheduling framework capable of capturing a large class of combinatorial constraints and preferences on resource space-time.\n","date":1620132907,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1620132907,"objectID":"b2b483fdcc4681483e7545bdf57228b4","permalink":"/author/alexey-tumanov/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/alexey-tumanov/","section":"authors","summary":"Alexey Tumanov is a tenure-track Assistant Professor of Computer Science at Georgia TEch since August 2019. His research interests include Systems for Machine Learning, specifically soft real-time ML support, resource management for distributed ML, distributed inference, distributed dataframe analytics, and latency-aware Neural Architecture Search (NAS).","tags":null,"title":"Alexey Tumanov","type":"authors"},{"authors":null,"categories":null,"content":"I’m a 2nd year PhD CS student at Georgia Tech. At SAIL, I\u0026rsquo;m working on building systems for ML Inference pipelines, efficient neural architecture search, and democratizing machine learning by innovating both at supply and demand of ML models.\n","date":1620132907,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1620132907,"objectID":"dea7226bb3a3c7e8c94a69b5b90de3c1","permalink":"/author/alind-khare/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/alind-khare/","section":"authors","summary":"I’m a 2nd year PhD CS student at Georgia Tech. At SAIL, I\u0026rsquo;m working on building systems for ML Inference pipelines, efficient neural architecture search, and democratizing machine learning by innovating both at supply and demand of ML models.","tags":null,"title":"Alind Khare","type":"authors"},{"authors":null,"categories":null,"content":"I’m a 2nd year MS CS student at Georgia Tech. At SAIL, I\u0026rsquo;m working on efficient neural architecture search, and democratizing machine learning through better frameworks, tools, and algorithms.\n","date":1619656722,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1619656722,"objectID":"ed85c7dadfbe88ce220ea16d6958322a","permalink":"/author/manas-sahni/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/manas-sahni/","section":"authors","summary":"I’m a 2nd year MS CS student at Georgia Tech. At SAIL, I\u0026rsquo;m working on efficient neural architecture search, and democratizing machine learning through better frameworks, tools, and algorithms.","tags":null,"title":"Manas Sahni","type":"authors"},{"authors":null,"categories":null,"content":"I am a 2nd year M.S student in Computer Science at Georgia Tech. I am interested in machine learning with focus on exploring compact deep neural network designs for efficient training and inference. Recently, I have been working on latency guided neural architecture search and cache designs for deep neural architectures.\n","date":1619656722,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1619656722,"objectID":"cc02c546c6ae51b86254ec9bd452359d","permalink":"/author/shreya-varshini/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/shreya-varshini/","section":"authors","summary":"I am a 2nd year M.S student in Computer Science at Georgia Tech. I am interested in machine learning with focus on exploring compact deep neural network designs for efficient training and inference.","tags":null,"title":"Shreya Varshini","type":"authors"},{"authors":null,"categories":null,"content":"I’m a 4th year BS CS and Mathematics student at Georgia Tech. At SAIL, I\u0026rsquo;m working on federated learning within healthcare applications with heterogeneous computational power and local data.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"5fd032867b39751cbd252ef32176707e","permalink":"/author/animesh-agrawal/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/animesh-agrawal/","section":"authors","summary":"I’m a 4th year BS CS and Mathematics student at Georgia Tech. At SAIL, I\u0026rsquo;m working on federated learning within healthcare applications with heterogeneous computational power and local data.","tags":null,"title":"Animesh Agrawal","type":"authors"},{"authors":null,"categories":null,"content":"I am a second year MS in ECE student at Georgia Tech. At SAIL, I\u0026rsquo;m working on reducing the inference latency of deep neural networks as a part of the Learned Caches project. Broadly, my interests lie in the areas of systems for deep learning, computer vision and efficient deep neural networks for embedded systems.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2fbde5ed082c25175c830a5cbff50122","permalink":"/author/hrishikesh-kale/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/hrishikesh-kale/","section":"authors","summary":"I am a second year MS in ECE student at Georgia Tech. At SAIL, I\u0026rsquo;m working on reducing the inference latency of deep neural networks as a part of the Learned Caches project.","tags":null,"title":"Hrishikesh Kale","type":"authors"},{"authors":null,"categories":null,"content":"I’m a 3rd year undergrad CS student at Georgia Tech. At SAIL, I\u0026rsquo;m working on SRTML and MODIN, mainly focusing on distributed systems. I have a deep passion for anything low-level and love programming in C and Python. I also love making art!\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"0b44cc8959a0afa08ce77b43fc495914","permalink":"/author/meghavarnika-budati/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/meghavarnika-budati/","section":"authors","summary":"I’m a 3rd year undergrad CS student at Georgia Tech. At SAIL, I\u0026rsquo;m working on SRTML and MODIN, mainly focusing on distributed systems. I have a deep passion for anything low-level and love programming in C and Python.","tags":null,"title":"Meghavarnika Budati","type":"authors"},{"authors":null,"categories":null,"content":"I’m a 4th year undergrad CS student at Georgia Tech. I am interested building and optimizing Machine Learning models and the systems for supporting it. At SAIL, I have been working on the SRTML project.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"856d70ea6d28000d8248383ae851c687","permalink":"/author/shiva-devarajan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/shiva-devarajan/","section":"authors","summary":"I’m a 4th year undergrad CS student at Georgia Tech. I am interested building and optimizing Machine Learning models and the systems for supporting it. At SAIL, I have been working on the SRTML project.","tags":null,"title":"Shiva Devarajan","type":"authors"},{"authors":null,"categories":null,"content":"I’m a 1st year undergrad CS student at Georgia Tech. I am interested building systems and low-level programming. At SAIL, I\u0026rsquo;m working on the SRTML project!\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"c5c52e375cdb4ccfdb4421375c545183","permalink":"/author/srikar-vanavasam/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/srikar-vanavasam/","section":"authors","summary":"I’m a 1st year undergrad CS student at Georgia Tech. I am interested building systems and low-level programming. At SAIL, I\u0026rsquo;m working on the SRTML project!","tags":null,"title":"Srikar Vanavasam","type":"authors"},{"authors":null,"categories":null,"content":"I’m a 1st year PhD CS student at Georgia Tech. At SAIL, I\u0026rsquo;m working on Systems for DataFrame Analytics and Machine Learning.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"5a854d6dd9fa015beb5e5c3c8e39f697","permalink":"/author/xiwen-zhang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/xiwen-zhang/","section":"authors","summary":"I’m a 1st year PhD CS student at Georgia Tech. At SAIL, I\u0026rsquo;m working on Systems for DataFrame Analytics and Machine Learning.","tags":null,"title":"Xiwen Zhang","type":"authors"},{"authors":null,"categories":null,"content":"I am a PhD student working with Prof. Chao Zhang and Prof. Alexey Tumanov on Machine Learning at Georgia Institute of Technology. My research focus on developing cutting-edge ML/DL algorithms learning from multi-modal time series data to make robust individualized predictions and recommendations for decision making. My research topics include but are not limited to Time Series Analysis, Deep Modeling, (Non-parametric) Bayesian Modeling, Causal Inference and their applications to healthcare.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"9f25cfac8daa87c87fcf5234f1e65615","permalink":"/author/yanbo-xu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/yanbo-xu/","section":"authors","summary":"I am a PhD student working with Prof. Chao Zhang and Prof. Alexey Tumanov on Machine Learning at Georgia Institute of Technology. My research focus on developing cutting-edge ML/DL algorithms learning from multi-modal time series data to make robust individualized predictions and recommendations for decision making.","tags":null,"title":"Yanbo Xu","type":"authors"},{"authors":["Shenda Hong","Yanbo Xu","Alind Khare","Satria Priambada","Kevin Maher","Alaa Aljiffry","Jimeng Sun","Alexey Tumanov"],"categories":null,"content":"hello\n","date":1620132907,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1620132907,"objectID":"f26157f2514e938e7a85089c9211fdd0","permalink":"/projects/holmes/","publishdate":"2021-05-04T08:55:07-04:00","relpermalink":"/projects/holmes/","section":"projects","summary":"In many clinical environments such as intensive care unit (ICU), real-time model serving is equally if not more important than accuracy. To make timely decisions, we argue the underlying serving system must be latency-aware. HOLMES is an online model ensemble serving framework for healthcare applications. HOLMES dynamically identifies the best performing set of models to ensemble for highest accuracy, while also satisfying sub-second latency constraints on end-to-end prediction.","tags":null,"title":"HOLMES: Health OnLine Model Ensemble Serving for Deep Learning Models in Intensive Care Units","type":"projects"},{"authors":["Manas Sahni","Shreya Varshini","Alind Khare","Alexey Tumanov"],"categories":[],"content":"Introduction If you’ve trained deep learning models, you know the process can take hours or days (weeks?) and thousands of dollars’ worth of computation. With increasing use of DNNs in common production, this problem only gets bigger – they need to be used on diverse deployment targets with widely varying latency constraints, based on hardware capabilities and application requirements. Designing DNN architectures that maximize accuracy under these constraints adds another degree of complexity requiring manual expertise and/or neural architecture search (NAS) – which are even slower and costlier than training. Clearly, repeating these processes for every deployment target is not scalable and therefore, solving this problem is essential for making DNNs easier to use in real deployment.\n  In CompOFA, we propose a cost-effective and faster technique to build model families that support multiple deployment platforms. Using insights from model design and system deployment, we build upon the current best methods that take 40-50 GPU days of computation and make their training and searching processes faster by 2x and 200x, respectively – all while building a family of equally efficient and diverse models!\nHow it’s done today   Conventional, individual training   The prevailing norm today is to build individual neural networks. We design and train single monolithic DNNs with a fixed accuracy and latency measure (or computational complexity, energy usage, etc.). Both, designing efficient architectures and training on production-grade datasets, require computation worth several GPU hours with slow turnaround, expensive hardwares and expertise in ML and Systems. In 2019, a study estimated the carbon emissions of one well-known NAS technique to be 283 metric tons – or nearly 60 times the emissions over an average human lifetime! Thus it is simply unscalable to continue this trend of designing and training individual DNNs for deployment.\n  Once-For-All (OFA): co-trained model families   Once-For-All (OFA) proposed to address this problem via weight-shared sub-networks of a larger network. These sub-networks of varying sizes had diverse accuracy and latency measures and could be trained simultaneously (rather than one-by-one). Post this one-time training, one can independently search and extract a subnetwork with optimal accuracy for a given deployment. Hence, OFA significantly improved the scalability over the naïve method. But, at 40-50 GPU days of train time, OFA remained expensive and required special training \u0026amp; searching procedures for its huge search space of $10^{19}$ models.\nIn CompOFA, we find insights that speed up OFA’s training and searching methodologies, while making it easier to use.\nCompOFA OFA built a model search space by slicing smaller subnetworks from a larger network – by choosing subsets of its layers (depth), channels (width), resolution, etc. This choice was made independently at each layer, contributing to a combinatorial explosion of $10^{10}-10^{19}$ models! These models don’t come free – training so many of them together needs a slow, phased approach. After training, the search also requires building special accuracy and latency estimators.\nBut do we need such a large search space?\n  Are all these models efficient? No! Many of these subnetworks are of dimensions that are suboptimal, lying well below the optimal accuracy-latency tradeoff.\n  Are all these models different enough? No! Imagine $10^{19}$ networks where the smallest and largest differ in latency by just 100ms – this fine granularity is too small to matter for real hardware.\n  In CompOFA, we question whether we can identify and focus our attention just on models that are close to optimal, and at a sufficient granularity? After all, it’s not common practice to treat these model dimensions as independent or random –- we often combine dimensions like depth and width to vary together i.e. in a compound fashion. This common intuition is backed by empirical studies like EfficientNet and RegNet, which showed that there are optimal relations between these dimensions.\n  CompOFA reduces combinatorial explosion of the search space by exploiting the same direction of growth of accuracy and latency.   Inspired by this, CompOFA uses a simple but powerful heuristic – choose models that grow depth and width together. This makes our search space much more tractable, but still just as efficient and diverse for actual use.\nIn our paper, we show that we can train this model family in half the time and all at once, without a slow phased approach. After training, we can search for models 216x faster, and without the time and effort to build special estimators.\n    Despite these savings, CompOFA does not compromise on its original goal. It’s able to extract networks for multiple latency targets on distinct hardware types, and match the existing SOTA in both optimality and range of its models.\n  CompOFA generates efficient model families for diverse hardwares \u0026ndash; from mobile phones to GPUs   Learn more CompOFA improves the speed, cost, and usability of jointly training models for many deployment targets. By highlighting insights on model design and system deployment, we try to address an important problem for real-world usability of DNNs.\nTo know more, please check out our paper and poster at ICLR 2021! Our code and pretrained models are also available on our Github repository.\nCitation @inproceedings{compofa-iclr21, author = {Manas Sahni and Shreya Varshini and Alind Khare and Alexey Tumanov}, title = {{C}omp{OFA}: Compound Once-For-All Networks for Faster Multi-Platform Deployment}, month = {May}, booktitle = {Proc. of the 9th International Conference on Learning Representations}, series = {ICLR '21}, year = {2021}, url = {https://openreview.net/forum?id=IgIk8RRT-Z} }  ","date":1619656722,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1619656722,"objectID":"a77238dbf9f860735172d1788c3c1e0a","permalink":"/post/compofa/","publishdate":"2021-04-28T20:38:42-04:00","relpermalink":"/post/compofa/","section":"post","summary":"CompOFA improves the speed, cost, and usability of jointly training models for many deployment targets. By highlighting insights on model design and system deployment, we try to address an important problem for real-world usability of DNNs.","tags":[],"title":"Introducing CompOFA","type":"post"},{"authors":["Manas Sahni","Shreya Varshini","Alind Khare","Alexey Tumanov"],"categories":null,"content":"   Conventional training, current SOTA, and CompOFA   The emergence of CNNs in mainstream deployment has necessitated methods to design and train efficient architectures tailored to maximize the accuracy under diverse hardware \u0026amp; latency constraints.\nDesigning and training DNN architectures for each deployment target is not feasible. Each deployment costs training time, compute dollars, system expertise, ML expertise, CO2 emissions.\nIn CompOFA, we propose a cost-effective and faster technique to build model families that support multiple deployment platforms. Using insights from model design and system deployment, we build upon the current best methods that take 40-50 GPU days of computation and make their training and searching processes faster by 2x and 200x, respectively – all while building a family of equally efficient and diverse models!\nCompOFA matches efficiency and diversity of SOTA methods\u0026hellip;   Efficient model families for diverse hardwares \u0026ndash; from mobile phones to GPUs   \u0026hellip;with 2x faster training and 216x faster searching   Better overall average accuracy At a population level, CompOFA has a higher concentration of accurate models\n  Learn more Please check out our paper and poster at ICLR 2021! Our code and pretrained models are also available on our Github repository. Also check out our blog post!\nCitation @inproceedings{compofa-iclr21, author = {Manas Sahni and Shreya Varshini and Alind Khare and Alexey Tumanov}, title = {{C}omp{OFA}: Compound Once-For-All Networks for Faster Multi-Platform Deployment}, month = {May}, booktitle = {Proc. of the 9th International Conference on Learning Representations}, series = {ICLR '21}, year = {2021}, url = {https://openreview.net/forum?id=IgIk8RRT-Z} }  ","date":1619294122,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1619294122,"objectID":"5ea965872b924d495ea7e7fc740e6cce","permalink":"/projects/compofa/","publishdate":"2021-04-24T15:55:22-04:00","relpermalink":"/projects/compofa/","section":"projects","summary":"CompOFA improves the speed, cost, and usability of jointly training models for many deployment targets. By highlighting insights on model design and system deployment, we try to address an important problem for real-world usability of DNNs.","tags":null,"title":"CompOFA: Compound Once-For-All Networks for Faster Multi-Platform Deployment","type":"projects"},{"authors":["admin"],"categories":null,"content":"Supplementary notes can be added here, including code and math.\n","date":1554595200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554595200,"objectID":"557dc08fd4b672a0c08e0a8cf0c9ff7d","permalink":"/publication/preprint/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/preprint/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example preprint / working paper","type":"publication"},{"authors":["admin","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.   Supplementary notes can be added here, including code and math.\n","date":1441065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441065600,"objectID":"966884cc0d8ac9e31fab966c4534e973","permalink":"/publication/journal-article/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/journal-article/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example journal article","type":"publication"},{"authors":["admin","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.   Supplementary notes can be added here, including code and math.\n","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372636800,"objectID":"69425fb10d4db090cfbd46854715582c","permalink":"/publication/conference-paper/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/conference-paper/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example conference paper","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6d99026b9e19e4fa43d5aadf147c7176","permalink":"/contact/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/contact/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c1d17ff2b20dca0ad6653a3161942b64","permalink":"/people/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/people/","section":"","summary":"","tags":null,"title":"","type":"widget_page"}]